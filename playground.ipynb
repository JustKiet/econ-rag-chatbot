{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv langchain_core langchain langchain_community langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "from typing import List, Dict, Any\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_preprocess(docs: List[Document]):\n",
    "    splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        encoding_name=\"cl100k_base\",\n",
    "        chunk_size=4000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "\n",
    "    text = splitter.split_documents(docs)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"Insert your OpenAI API key here\"\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, \n",
    "                 file_path: str = None, \n",
    "                 model: str = \"gpt-4o-mini\", \n",
    "                 temperature: float = 0,\n",
    "                 splitter_encoding_name: str = \"cl100k_base\", \n",
    "                 embedding_model: str = \"text-embedding-3-large\",\n",
    "                 chunk_size: int = 4000,\n",
    "                 chunk_overlap: int = 200,\n",
    "                 vectorstore: FAISS = None) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a RAGPipeline object with a file path to a PDF document.\\n\n",
    "        Init params:\n",
    "        - ``file_path``: **str** -> Path to the PDF document.\n",
    "        - ``model``: **Optional[str]** -> OpenAI model name. *(default: \"gpt-4o-mini\")*\n",
    "        - ``temperature``: **Optional[float]** -> OpenAI temperature. *(default: 0)*\n",
    "        - ``splitter_encoding_name``: **Optional[str]** -> CharacterTextSplitter encoding name. *(default: \"cl100k_base\")*\n",
    "        - ``embedding_model``: **Optional[str]** -> OpenAI embedding model name. *(default: \"text-embedding-3-large\")*\n",
    "        - ``chunk_size``: **Optional[int]** -> Chunk size for the PDF document. *(default: 4000)*\n",
    "        - ``chunk_overlap``: **Optional[int]** -> Chunk overlap for the PDF document. *(default: 200)*\n",
    "        - ``vectorstore``: **Optional[FAISS]** -> FAISS vectorstore object. *(default: None)*\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup splitter hyperparameters\n",
    "        self.SPLITTER_ENCODING_NAME = splitter_encoding_name\n",
    "        self.CHUNK_SIZE = chunk_size\n",
    "        self.CHUNK_OVERLAP = chunk_overlap\n",
    "        \n",
    "        # Set the OpenAI embeddings model\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                model=embedding_model,\n",
    "        )\n",
    "        \n",
    "        # Load the PDF document\n",
    "        if file_path is not None:\n",
    "            self.document = PyPDFLoader(file_path).load()\n",
    "\n",
    "            self.text = self.pdf_preprocess()\n",
    "\n",
    "        self.model = ChatOpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Create a FAISS vectorstore object\n",
    "        if vectorstore is None:\n",
    "            self.vectorstore = FAISS.from_documents(\n",
    "                documents=self.text,\n",
    "                embedding=self.embeddings,\n",
    "            )\n",
    "        else:\n",
    "            self.vectorstore = vectorstore\n",
    "\n",
    "        # Load the retrieval and chat models\n",
    "        # Define the system basic RAG prompt for model\n",
    "        self.system_prompt = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Use the following pieces of retrieved context to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know. Use three sentences maximum and keep the \"\n",
    "            \"answer concise.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "\n",
    "        # Define the ChatPromptTemplate for the model which includes SystemMessage, ChatHistory and HumanMessage\n",
    "        self.prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Save conversation history and contextualize future questions\n",
    "        # Define the system prompt for contextualizing previous interactions\n",
    "        self.contextualize_system_prompt = (\n",
    "            \"Given a chat history and the latest user question \"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question which can be understood \"\n",
    "            \"without the chat history. Do NOT answer the question, \"\n",
    "            \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        )\n",
    "\n",
    "        self.contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.contextualize_system_prompt), \n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.contextualized_model = self.model.with_config(tags=[\"contextualized_model\"])\n",
    "\n",
    "        self.history_aware_retriever = create_history_aware_retriever(\n",
    "            self.contextualized_model,\n",
    "            self.vectorstore.as_retriever(),\n",
    "            self.contextualize_prompt,\n",
    "        )\n",
    "\n",
    "        self.combine_docs_chain = create_stuff_documents_chain(self.model, self.prompt)\n",
    "        self.chain = create_retrieval_chain(self.history_aware_retriever, self.combine_docs_chain)\n",
    "\n",
    "        self.chat_history = []\n",
    "\n",
    "    \n",
    "    def invoke_(self, query: str):\n",
    "        \"\"\"\n",
    "        Execute the RAGPipeline with a query.\\n\n",
    "        **Method:**\n",
    "        - Execute the RAGPipeline chain with the query.\n",
    "        - Return the response.\n",
    "        \"\"\"\n",
    "        response = self.chain.invoke({\"input\": query, \"chat_history\": self.chat_history})\n",
    "        self.chat_history.extend(\n",
    "            [\n",
    "                HumanMessage(content=query),\n",
    "                AIMessage(content=response[\"answer\"]),\n",
    "            ]\n",
    "        )\n",
    "        return response\n",
    "    \n",
    "    def stream_(self, query: str):\n",
    "        \"\"\"\n",
    "        Execute the RAGPipeline with a query (stream enabled).\\n\n",
    "        **Method:**\n",
    "        - Execute the RAGPipeline chain with the query.\n",
    "        - Stream the response.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = []\n",
    "        \n",
    "        for chunk in self.chain.stream({\"input\": query, \"chat_history\": self.chat_history}):\n",
    "            if 'answer' in chunk:\n",
    "                response.append(chunk['answer'])\n",
    "\n",
    "                print(f\"{chunk['answer']}\", end=\"\")\n",
    "                                \n",
    "        return \"\".join(response)\n",
    "    \n",
    "    async def astream_(self, query: str):\n",
    "        \"\"\"\n",
    "        Execute the RAGPipeline with a query (async stream enabled).\\n\n",
    "        **Method:**\n",
    "        - Execute the RAGPipeline chain with the query.\n",
    "        - Stream the response asynchronously.\n",
    "        \"\"\"\n",
    "        response = []\n",
    "        async for event in self.chain.astream_events(\n",
    "            {\"input\": query, \"chat_history\": self.chat_history}, \n",
    "            version=\"v2\",\n",
    "        ):\n",
    "            if (event[\"event\"] == \"on_chat_model_stream\" and \"contextualized_model\" in event[\"tags\"]):\n",
    "                response_chunk = event[\"data\"][\"chunk\"]\n",
    "                response.extend(response_chunk.content)\n",
    "                print(f\"{response_chunk.content}\", end=\"\")\n",
    "                                \n",
    "        return \"\".join(response)\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def from_local(cls, \n",
    "                   folder_path: str,\n",
    "                   model: str = \"gpt-4o-mini\",\n",
    "                   temperature: float = 0,\n",
    "                   embedding_model: str = \"text-embedding-3-large\"):\n",
    "        \"\"\"\n",
    "        Load a RAGPipeline from a local **FAISS vectorstore** folder path.\\n\n",
    "        Example FAISS vectorstore folder structure:\n",
    "        ```\n",
    "        ./src\n",
    "        |___vectorstore:\n",
    "        |   |   index.faiss\n",
    "        |   |   index.pkl\n",
    "        ```\n",
    "        \"\"\"\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            model=embedding_model,\n",
    "        )\n",
    "\n",
    "        vectorstore = FAISS.load_local(\n",
    "            folder_path=folder_path,\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "\n",
    "        return cls(vectorstore=vectorstore, \n",
    "                   model=model, \n",
    "                   temperature=temperature, \n",
    "                   embedding_model=embedding_model)\n",
    "\n",
    "    # Preprocess the PDF document\n",
    "    def pdf_preprocess(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Preprocess the PDF document using CharacterTextSplitter (WIP).\\n\n",
    "        **Method:**\n",
    "        - Initialize a [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html) object.\n",
    "        - Parse the **Document** Object to the splitter with default params: ``encoding_name=\"cl100k_base\"``, ``chunk_size=4000`` and ``overlap=200``.\n",
    "\n",
    "        **Output**\n",
    "        - **List[Document]**: List of Document objects.\n",
    "\n",
    "        **Todo:**\n",
    "        - Implement a way to parse tables and equations from the PDF document.\n",
    "        \"\"\"\n",
    "        splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "            encoding_name=self.SPLITTER_ENCODING_NAME,\n",
    "            chunk_size=self.CHUNK_SIZE,\n",
    "            chunk_overlap=self.CHUNK_OVERLAP,\n",
    "        )\n",
    "\n",
    "        splits = splitter.split_documents(self.document)\n",
    "\n",
    "        return splits\n",
    "    \n",
    "    # Save the FAISS vectorstore to a local folder\n",
    "    def save_vectorstore(self, path: str) -> str:\n",
    "        \"\"\"\n",
    "        Save the FAISS vectorstore to a local folder.\n",
    "        \"\"\"\n",
    "        self.vectorstore.save_local(path)\n",
    "        return f\"Saved vectorstore to {path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAGPipeline.from_local(\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saved vectorstore to ./vectorstore'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.save_vectorstore(\"./vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Sách này là gì?',\n",
       " 'chat_history': [HumanMessage(content='Sách này là gì?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Sách này là giáo trình \"Phân tích báo cáo tài chính\" do Nhà xuất bản Đại học Kinh tế Quốc dân phát hành năm 2013. Chủ biên của giáo trình là PGS. TS. Nguyễn Năng Phúc. Nó được sử dụng trong lĩnh vực kế toán và tài chính.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Sách này là gì?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Sách này là giáo trình \"Phân tích báo cáo tài chính\" do Nhà xuất bản Đại học Kinh tế Quốc dân phát hành năm 2013. Chủ biên của giáo trình là PGS. TS. Nguyễn Năng Phúc. Nó phục vụ cho việc học tập và nghiên cứu trong lĩnh vực kế toán và tài chính.', additional_kwargs={}, response_metadata={})],\n",
       " 'context': [Document(metadata={'source': './docs.pdf', 'page': 360}, page_content='GIÁO TRÌNH\\nP H Â N  TÍCH  B Á O  C Á O  T À I C H ÍN H\\nNHÀ XUẤT BẢN ĐẠI HỌC KINH TẾ QUỐC DAN\\nĐịa chỉ: 207 Đường Giải Phóng, Hà Nội \\nWebsite: http://nxb.neu.edu.vn-Email:  nxb@neu.edu.vn \\nĐịa chỉ phát hành Ebooks: http://alezaa.com/ktqd \\nĐiện thoại: (04) 38696407-36282486-36282483 \\nFax: (04)36282485\\nBO  L J  C 8\\nChịu trách nhiệm xuất bản:  GS.TS. N G U Y Ễ N  T H À N H  Đ Ộ , Giám  đốc, Tổng bién tập\\nC h ịu  trách nhiệm  nội dung:\\nBiên tập kỹ thuật:\\nC h ế  bản vi tinh:\\nThiết k ế  bìa:\\nSửa bản in và đ ọ c sá ch  mẫu:PGS. TS. NG UYỄN N À N G  PH ÚC\\nNGỌC LAN - TRỊNH QUYÊN  \\nNGUYỄN LAN  \\nTRẤN MAI HOA  \\nNGỌC LAN - TRỊNH QUYÊN\\nIn 1.000cuốn,khổ 16x24cm tại XuởnginTruờngĐHKTQDvàCôngtyinPhúThịnh  \\nMã số ĐKXB: 75 -2013/CXB/199 -232/DHKTQD và ISBN: 978-604-927-451-0.  \\nSô quyết định xuất bản: 136/QĐ-NXBĐHKTQD  \\nIn xong và nộp lưu chiểu quý IV năm 2013.'),\n",
       "  Document(metadata={'source': './docs.pdf', 'page': 359}, page_content='30. Wguyên lý kế toán (Lý thuyết và bài tập),  NXB Thống kẽ, Hà Nội (r$m\\n2008), Tác già: Phan Đức Dũng\\n31. Kế toán chi phí giả thành (Lỷ thuyết và bài tập),  NXB Thống Kê, Hà \\nNội (năm 2008), Tác giả: Phan Đức Dũng.\\n32. Kế toán quàn trị (Lý thuyết và bài tập).  NXB Thống kê, Hà Nội (năm\\n2008), Tác già: Phan Đức Dũng.\\n33. Thông tư I6I/2007/TT-BTC ngày 31/12/2007  của Bộ Tài chính\\n34. Ké toán quan trị, NXB Tài chính,  Hà Nội năm 2009, Tác già: PGS.TS. \\nNguyễn Ngọc Quang.\\n35 GT- Phân tích hoạt động kinh doanh,  NXB Thời đại, Hà Nội năm 2010. \\nChủ biên, PGS.TS. Nguyễn Năng Phúc.\\n359'),\n",
       "  Document(metadata={'source': './docs.pdf', 'page': 358}, page_content='15. Chế độ kế toán doanh nghiệp,  Quyền 1, NXB Tài chính, Hà Nội (2001 ))\\n16. Ché độ kế toán doanh nghiệp, quyến 2,  NXB Tài chính, Hà Nội nàn \\n2006.\\n17. GT Lý thuyết hạch toán kế toán,  NXB Đại học KTQD, Hà Nội (nàn\\n2007). Chù biên: PGS.TS. Nguyễn Thị Đông.\\n18. Phân tích tài chinh trong các công ty cố phần Việt Nam,  NXB Tài \\nchính, Hà Nội (năm 2004), Tác già: PGS.TS. Nguyễn Năng Phúc.\\n19. Phân tích tài chinh công ty cô phần,  NXB Tài Chính, Hà Nội (năn \\n2006), Chù biên: PGS.TS. Nguyễn Năng Phúc.\\n20. Phán tích hoạt động đầu tư tài chính cùa docrntì nghiệp,  NXB Tìi \\nchính, Hà Nội (2005) Tác già: PGS.TS. Nguyễn Năng Phúc.\\n21! Lập, đọc, kiêm Ira và phân tích báo cáo tài chính,  NXB Tài chính, hà \\nNội (2002), Chù biên: TS. Nguyễn Văn Công.\\n22. Kế toán quàn trị doanh nghiệp,  NXB Tài chính, Hà Nội (năm 200Ỉ) \\nTác già: PGS.TS. Nguyễn Năng Phúc.\\n23. Ke Ioán quàn trị,  NXB Đại học Quốc gia TP.HCM, (năm 2003), TtC \\ngiả: Nguyễn Tấn Bình.\\n24. Phán tích hoạt động kinh doanh,  NXB Thống Kê, Hà Nội (năm 2004), \\nTác giả: Nguyễn Tấn Bình\\n25. c. Mác Tư bàn quyến II, tập  /, NXB Sự thật, Hà Nội (năm 1959)\\n26. GT Quán trị kinh doanh,  NXB Lao động - Xã hội, Hà Nội (năm 2004|, \\nChủ biên: GS.TS. Nguyễn Thành Độ.\\n27. GT Phân tích hoạt động . kinh doanh,  NXB Giáo dục, Hà Nội (năn \\n2004), Chủ biên: PGS.TS. Phạm Thị Gái.\\n28. Phán tích hoạt động kinh doanh,  NXB Giáo dục Hà Nội (năm 2001 1, \\n,T*ác giả: TS. Trương Bá Thanh và Ths. Trần Đình Khôi Nguyên.\\n29. Giáo trình Thống kê kinh tế,  NXB Giáo dục, Hà Nội (năm 2002) Chi \\nbiên: TS. Phan Công Nghĩa.\\n358'),\n",
       "  Document(metadata={'source': './docs.pdf', 'page': 0}, page_content='JG ĐẠI HỌC KINH TỂ QUỐC DÂN \\nKHOA KÊ TOÁN\\nChủ biên: PGS. TS. Nguyên Năng Phúc\\nGiáo trình\\nPH A N  TICH  \\nBÁO CÁO  \\nTÀI CHÍNH\\nNHÀ XUÂT BÁN ĐẠI HỌC KINH TỂ Q U Ỗ C DÂN \\n2013JYÊN\\nIỆU')],\n",
       " 'answer': 'Sách này là giáo trình \"Phân tích báo cáo tài chính\" do Nhà xuất bản Đại học Kinh tế Quốc dân phát hành năm 2013. Chủ biên của giáo trình là PGS. TS. Nguyễn Năng Phúc. Nó phục vụ cho việc học tập và nghiên cứu trong lĩnh vực kế toán và tài chính.'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QA with output as dict (metadata)\n",
    "invoke_response = retriever.invoke_(\"Sách này là gì?\")\n",
    "invoke_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sách này là giáo trình \"Phân tích báo cáo tài chính\", nhằm cung cấp thông tin cần thiết để đánh giá sức mạnh tài chính, khả năng sinh lời và triển vọng phát triển của doanh nghiệp. Nó phục vụ cho nhiều đối tượng như nhà quản lý, nhà đầu tư, và sinh viên kinh tế, với nội dung được biên soạn dựa trên tài liệu trong nước và quốc tế."
     ]
    }
   ],
   "source": [
    "# QA with output as dictionary\n",
    "stream_response = retriever.stream_(\"Sách này nói về gì?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
